{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Document Ingestion and Indexing for Medical RAG\n",
    "\n",
    "This notebook demonstrates the first part of building our Retrieval Augmented Generation (RAG) system:\n",
    "1. Loading PDF documents from a specified folder.\n",
    "2. Chunking the documents into smaller, manageable nodes.\n",
    "3. Generating embeddings for these nodes.\n",
    "4. Building a FAISS vector index and persisting it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index faiss-cpu sentence-transformers pypdf2\n",
    "%pip install llama-index-embeddings-huggingface \n",
    "# %pip install llama-index-readers-file # If SimpleDirectoryReader needs it explicitly\n",
    "\n",
    "print(\"Dependencies installed. You may need to restart the kernel for changes to take effect.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to Python path to import custom modules\n",
    "# Assuming the notebook is in 'medical_rag_agent/notebooks/' and src is in 'medical_rag_agent/src/'\n",
    "module_path = str(Path.cwd().parent / 'src')\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from document_processor import load_pdfs_from_folder, chunk_documents\n",
    "from indexing import get_embedding_model, build_vector_index, load_vector_index\n",
    "\n",
    "# Define paths\n",
    "# The data_folder should point to 'medical_rag_agent/data/sample_medical_pdfs/' relative to the project root\n",
    "# The notebook is in 'medical_rag_agent/notebooks/', so '../data/sample_medical_pdfs/'\n",
    "data_folder = '../data/sample_medical_pdfs/'\n",
    "# The vector_store_path should be relative to the project root as well.\n",
    "# Let's save it in 'medical_rag_agent/vector_store_notebook/'\n",
    "vector_store_notebook_path = '../vector_store_notebook' \n",
    "\n",
    "print(f\"Data folder: {Path(data_folder).resolve()}\")\n",
    "print(f\"Vector store path for this notebook: {Path(vector_store_notebook_path).resolve()}\")\n",
    "print(f\"Python sys.path includes: {module_path}\")\n",
    "\n",
    "# Create vector store directory if it doesn't exist\n",
    "os.makedirs(vector_store_notebook_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load PDF Documents\n",
    "\n",
    "We'll use the `load_pdfs_from_folder` function from our `document_processor` module.\n",
    "This function expects a path to a folder containing PDF files.\n",
    "We've prepared a sample PDF in `data/sample_medical_pdfs/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_pdfs_from_folder(data_folder)\n",
    "if documents:\n",
    "    print(f\"Successfully loaded {len(documents)} document(s).\")\n",
    "    for doc in documents:\n",
    "        print(f\"Document ID: {doc.doc_id}, File: {doc.metadata.get('file_name', 'N/A')}\")\n",
    "        # print(f\"First 100 chars: {doc.text[:100]}...\") # Optional: print snippet\n",
    "else:\n",
    "    print(\"No documents found or loaded. Please check the data_folder path and its contents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chunk Documents\n",
    "\n",
    "Next, we split the loaded documents into smaller chunks (nodes). Our `chunk_documents` function now attempts to identify common medical/scientific section headers (e.g., 'Introduction', 'Methods', 'Results', 'Diagnosis', 'Treatment Plan') to create more semantically relevant chunks. If a section is too large, or if no headers are found, it falls back to sentence-based splitting within those sections (or the whole document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if documents: # Proceed only if documents were loaded\n",
    "    # Using new default parameters from the updated chunk_documents function\n",
    "    # nodes = chunk_documents(documents, default_chunk_size=512, default_chunk_overlap=100, max_chars_per_section_chunk=3000)\n",
    "    nodes = chunk_documents(documents) # Using the function's defaults\n",
    "    print(f\"Successfully chunked {len(documents)} document(s) into {len(nodes)} nodes.\")\n",
    "    if nodes:\n",
    "        print(f\"Example Node 1 ID: {nodes[0].id_}\")\n",
    "        print(f\"Example Node 1 Text (first 100 chars): {nodes[0].get_content()[:100]}...\")\n",
    "        print(f\"Example Node 1 Metadata (file_name): {nodes[0].metadata.get('file_name', 'N/A')}\")\n",
    "        print(f\"Example Node 1 Metadata (section): {nodes[0].metadata.get('section', 'N/A')}\")\n",
    "        if len(nodes) > 1: # Print another node if available to see variety\n",
    "              print(f\"Example Node 2 ID: {nodes[1].id_}\")\n",
    "              print(f\"Example Node 2 Text (first 100 chars): {nodes[1].get_content()[:100]}...\")\n",
    "              print(f\"Example Node 2 Metadata (file_name): {nodes[1].metadata.get('file_name', 'N/A')}\")\n",
    "              print(f\"Example Node 2 Metadata (section): {nodes[1].metadata.get('section', 'N/A')}\")\n",
    "else:\n",
    "    print(\"Skipping chunking as no documents were loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Embedding Model\n",
    "\n",
    "We'll use a pre-trained model from Hugging Face via `llama-index.embeddings.HuggingFaceEmbedding`.\n",
    "The `get_embedding_model` function from `indexing.py` handles this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embed_model = get_embedding_model(model_name=embed_model_name)\n",
    "print(f\"Successfully initialized embedding model: {embed_model_name}\")\n",
    "if hasattr(embed_model, 'embed_dim'):\n",
    "    print(f\"Embedding dimension: {embed_model.embed_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build and Save Vector Index\n",
    "\n",
    "Now we combine the chunked nodes and the embedding model to build our FAISS vector index.\n",
    "The `build_vector_index` function will store the index files in the `vector_store_notebook_path` directory.\n",
    "This process can take a few moments depending on the number of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'nodes' in locals() and nodes and embed_model: # Proceed only if nodes and model are available\n",
    "    print(f\"Building vector index. This may take a moment...\")\n",
    "    # Make sure the path used here is where you want the Colab notebook's index to be stored.\n",
    "    index = build_vector_index(nodes, embed_model, storage_persist_dir=vector_store_notebook_path)\n",
    "    print(f\"Successfully built and saved the vector index to: {vector_store_notebook_path}\")\n",
    "    \n",
    "    # Verify that files were created\n",
    "    expected_faiss_file = Path(vector_store_notebook_path) / \"vector_store.faiss\"\n",
    "    expected_docstore_file = Path(vector_store_notebook_path) / \"docstore.json\"\n",
    "    if expected_faiss_file.exists() and expected_docstore_file.exists():\n",
    "        print(f\"Verified: FAISS file exists at {expected_faiss_file}\")\n",
    "        print(f\"Verified: Docstore file exists at {expected_docstore_file}\")\n",
    "    else:\n",
    "        print(f\"Warning: Could not verify all index files in {vector_store_notebook_path}. Check build_vector_index implementation.\")\n",
    "        print(f\"Contents of {vector_store_notebook_path}: {list(Path(vector_store_notebook_path).iterdir())}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping index building as nodes or embedding model are not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Loading the Index (Optional)\n",
    "\n",
    "To ensure persistence worked correctly, let's try loading the index back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'index' in locals() and embed_model and Path(vector_store_notebook_path).exists():\n",
    "    try:\n",
    "        print(f\"Attempting to load index from: {vector_store_notebook_path}\")\n",
    "        loaded_index = load_vector_index(storage_persist_dir=vector_store_notebook_path, embed_model=embed_model)\n",
    "        print(\"Successfully loaded index from disk.\")\n",
    "        # You can try a sample query if you had an LLM configured, but for now, just loading is fine.\n",
    "        # Example: query_engine = loaded_index.as_query_engine()\n",
    "        # response = query_engine.query(\"What is RAG?\")\n",
    "        # print(response)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading index: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"Skipping index loading test as prerequisites are not met.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "With the index built and saved, the next stage involves setting up a query engine and then an LLM agent to interact with this indexed medical data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
